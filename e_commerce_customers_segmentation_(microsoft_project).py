# -*- coding: utf-8 -*-
"""E-commerce Customers Segmentation (Microsoft project)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V95Qemz5RdX-b8l8ETLtS7uJ4pbmnkoI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#from google.colab import files
#uploaded = files.upload()

file_path = "/content/E-commerce_data.xlsx"
xls = pd.ExcelFile(file_path)
print(xls.sheet_names)

customers = pd.read_excel(xls, sheet_name="customers")
genders = pd.read_excel(xls, sheet_name="genders")
cities = pd.read_excel(xls, sheet_name="cities")
transactions = pd.read_excel(xls, sheet_name="transactions")
branches = pd.read_excel(xls, sheet_name="branches")
merchants = pd.read_excel(xls, sheet_name="merchants")

"""## **Data Preprocessing**"""

# Merge Customers with Genders and Cities
customers = customers.merge(genders, on="gender_id", how="left")
customers = customers.merge(cities, on="city_id", how="left")

# Merge Transactions with Customers
transactions = transactions.merge(customers, on="customer_id", how="left")

# Convert dates to datetime
transactions["transaction_date"] = pd.to_datetime(transactions["transaction_date"])
transactions["burn_date"] = pd.to_datetime(transactions["burn_date"])

# Create a feature for transaction frequency per customer
customer_txn_count = transactions.groupby("customer_id")["transaction_id"].count().reset_index()
customer_txn_count.columns = ["customer_id", "transaction_count"]

print(customer_txn_count.head())

print(transactions.columns)

# Merge transaction frequency back to customers
customers = customers.merge(customer_txn_count, on="customer_id", how="left")
customers["transaction_count"] = customers["transaction_count"].fillna(0)

customers.head()

"""## **Feature Selection**"""

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Encode categorical variables (gender, city)
label_enc = LabelEncoder()

# Select features for clustering
features = customers[["transaction_count", "gender_id", "city_id"]]

# Normalize the data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

customers.head()

"""## **Customer Segmentation using K-Means**"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

wcss = []

print("WCSS values for different k:")
for k in range(1, 11):  # Testing k from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    wcss.append(kmeans.inertia_)  # Inertia = WCSS
    print(f"k = {k}, WCSS = {kmeans.inertia_}")

# Plot WCSS vs. k
plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("WCSS")
plt.title("Elbow Method for Optimal k")
plt.show()

# Fit K-Means with optimal K (letâ€™s assume 3 clusters based on elbow method)
kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)
customers["cluster"] = kmeans.fit_predict(scaled_features)

"""## **Model Evaluation**"""

from sklearn.metrics import silhouette_score

best_k = 2
best_score = -1

for k in range(2, 11):  # Avoid k=1 because silhouette score is not valid for one cluster
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(scaled_features)
    score = silhouette_score(scaled_features, cluster_labels)

    if score > best_score:
        best_score = score
        best_k = k

print(f"Best k: {best_k} with Silhouette Score: {best_score}")

"""## **Segment Analysis**"""

import seaborn as sns

# Visualizing clusters
sns.boxplot(x="cluster", y="transaction_count", data=customers)
plt.title("Transaction Count by Customer Segment")
plt.show()

# Count of customers in each segment
print(customers["cluster"].value_counts())

# Analyze cluster characteristics
cluster_analysis = customers.groupby("cluster").agg({
    "transaction_count": "mean",
    "gender_id": "first",
    "city_id": "first"
}).reset_index()

cluster_analysis